{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import random\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GOOOOOD IT WORKS!!!\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"user-data-dir=C:/Users/znay/AppData/Local/Google/Chrome/User Data\")\n",
    "options.add_argument(r'--profile-directory=Profile 6') #e.g. Profile 3\n",
    "options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "options.add_experimental_option(\"excludeSwitches\",[\"enable logging\"])\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable automation\"])\n",
    "options.add_argument(\"start-maximized\")\n",
    "\n",
    "driver = webdriver.Chrome(service = Service(executable_path=r\"C:\\Users\\znay\\Downloads\\chromedriver.exe\"), options = options)\n",
    "\n",
    "#driver.get(r\"https://medium.com/swlh/five-simple-ways-to-drastically-boost-your-energy-26f5ec19ad81\")\n",
    "\n",
    "#m = driver.find_element(By.XPATH, \"/html/body\").text\n",
    "\n",
    "#soup = BeautifulSoup(m, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "options = Options()\n",
    "options.add_argument(\"user-data-dir=C:/Users/znay/AppData/Local/Google/Chrome/User Data\")\n",
    "options.add_argument(r'--profile-directory=Profile 6') #e.g. Profile 3\n",
    "options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "options.add_experimental_option(\"excludeSwitches\",[\"enable logging\"])\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable automation\"])\n",
    "options.add_argument(\"start-maximized\")\n",
    "\n",
    "driver = webdriver.Chrome(service = Service(executable_path=r\"C:\\Users\\znay\\Downloads\\chromedriver.exe\"), options = options)\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\n",
    "def scraper(ogurl, year,csv_save_url):\n",
    "\n",
    "    stories_data = []\n",
    "\n",
    "    for month in range(1, 13):\n",
    "        if month in [1, 3, 5, 7, 8, 10, 12]:\n",
    "            n_days = 31\n",
    "        elif month in [4, 6, 9, 11]:\n",
    "            n_days = 30\n",
    "        else:\n",
    "            n_days = 28\n",
    "\n",
    "        for day in range(1, n_days + 1):\n",
    "\n",
    "            month, day = str(month), str(day)\n",
    "\n",
    "            if len(month) == 1:\n",
    "                month = f'0{month}'\n",
    "            if len(day) == 1:\n",
    "                day = f'0{day}'\n",
    "\n",
    "            date = f'{month}/{day}/{year}'\n",
    "            url = f'{ogurl}/archive/{year}/{month}/{day}'\n",
    "            print(url)\n",
    "            page = requests.get(url)\n",
    "            soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "            stories = soup.find_all('div', class_='streamItem streamItem--postPreview js-streamItem')\n",
    "            j=0\n",
    "            for raw_story in stories:\n",
    "                j=0\n",
    "                try:\n",
    "                    each_story = []\n",
    "\n",
    "                    match = re.findall(r'data-action-source=\"preview-listing\" data-action-value=\"(.*?=collection_archive)', str(raw_story))\n",
    "                    driver.get(match)\n",
    "\n",
    "                    text = driver.find_element(By.XPATH, \"/html/body\").text\n",
    "\n",
    "                    story = BeautifulSoup(text, 'html.parser')\n",
    "\n",
    "                    author_box = story.find('div', class_='postMetaInline u-floatLeft u-sm-maxWidthFullWidth')\n",
    "                    author_url = author_box.find('a')['href']\n",
    "\n",
    "                    try:\n",
    "                        reading_time = author_box.find('span', class_='readingTime')['title']\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "                    title = story.find('h3').text if story.find('h3') else '-'\n",
    "\n",
    "                    subtitle = story.find('h4').text if story.find('h4') else '-'\n",
    "\n",
    "                    if story.find('button', class_='button button--chromeless u-baseColor--buttonNormal'\n",
    "                                               ' js-multirecommendCountButton u-disablePointerEvents'):\n",
    "\n",
    "                        claps = story.find('button', class_='button button--chromeless u-baseColor--buttonNormal'\n",
    "                                                        ' js-multirecommendCountButton u-disablePointerEvents').text\n",
    "                    else:\n",
    "                        claps = 0\n",
    "\n",
    "                    if story.find('a', class_='button button--chromeless u-baseColor--buttonNormal'):\n",
    "                        responses = story.find('a', class_='button button--chromeless u-baseColor--buttonNormal').text\n",
    "                    else:\n",
    "                        responses = '0 responses'\n",
    "\n",
    "                    story_url = story.find('a', class_='button button--smaller button--chromeless u-baseColor--buttonNormal')[\n",
    "                    'href']\n",
    "\n",
    "                    reading_time = reading_time.split()[0]\n",
    "                    responses = responses.split()[0]\n",
    "\n",
    "                    story_page = requests.get(story_url)\n",
    "                    story_soup = BeautifulSoup(story_page.text, 'html.parser')\n",
    "\n",
    "                    sections = story_soup.find_all('section')\n",
    "                    story_paragraphs = []\n",
    "                    section_titles = []\n",
    "                    for section in sections:\n",
    "                        paragraphs = section.find_all('p')\n",
    "                        for paragraph in paragraphs:\n",
    "                            story_paragraphs.append(paragraph.text)\n",
    "\n",
    "                        subs = section.find_all('h1')\n",
    "                        for sub in subs:\n",
    "                            section_titles.append(sub.text)\n",
    "\n",
    "                    number_sections = len(section_titles)\n",
    "                    number_paragraphs = len(story_paragraphs)\n",
    "\n",
    "                    each_story.append(date)\n",
    "                    each_story.append(title)\n",
    "                    each_story.append(subtitle)\n",
    "                    each_story.append(claps)\n",
    "                    each_story.append(responses)\n",
    "                    each_story.append(author_url)\n",
    "                    each_story.append(story_url)\n",
    "                    each_story.append(reading_time)\n",
    "                    each_story.append(number_sections)\n",
    "                    each_story.append(section_titles)\n",
    "                    each_story.append(number_paragraphs)\n",
    "                    each_story.append(story_paragraphs)\n",
    "\n",
    "                    stories_data.append(each_story)\n",
    "                except:\n",
    "                    j+=1\n",
    "                    continue\n",
    "\n",
    "\n",
    "            print(f'{len(stories)-j} stories scraped on {date}.')\n",
    "\n",
    "    columns = ['date', 'title', 'subtitle', 'claps', 'responses', 'author_url', 'story_url',\n",
    "               'reading_time (mins)', 'number_sections', 'section_titles', 'number_paragraphs', 'paragraphs']\n",
    "\n",
    "    df = pd.DataFrame(stories_data, columns=columns)\n",
    "    df.to_csv(csv_save_url, sep='\\t', index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(r\"https://towardsdatascience.com//archive/2023/01/01\")\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "stories  =  soup.find_all('div', class_='streamItem streamItem--postPreview js-streamItem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"postMetaInline u-floatLeft u-sm-maxWidthFullWidth\"><div class=\"u-flexCenter\"><div class=\"postMetaInline-avatar u-flex0\"><a class=\"link u-baseColor--link avatar\" data-action=\"show-user-card\" data-action-type=\"hover\" data-action-value=\"4d4f8ddd1e68\" data-collection-slug=\"towards-data-science\" data-user-id=\"4d4f8ddd1e68\" dir=\"auto\" href=\"https://towardsdatascience.com/@mgalkin\"><img alt=\"Go to the profile of Michael Galkin\" class=\"avatar-image u-size36x36 u-xs-size32x32\" src=\"https://cdn-images-1.medium.com/fit/c/72/72/2*R6303tLavDAf6jJAsMlaJQ.jpeg\"/></a></div><div class=\"postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis\"><a class=\"ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken\" data-action=\"show-user-card\" data-action-source=\"collection_archive---------0-----------------------\" data-action-type=\"hover\" data-action-value=\"4d4f8ddd1e68\" data-collection-slug=\"towards-data-science\" data-user-id=\"4d4f8ddd1e68\" dir=\"auto\" href=\"https://towardsdatascience.com/@mgalkin?source=collection_archive---------0-----------------------\">Michael Galkin</a> in <a class=\"ds-link ds-link--styleSubtle link--darken link--accent u-accentColor--textNormal\" data-action=\"show-collection-card\" data-action-source=\"collection_archive---------0-----------------------\" data-action-type=\"hover\" data-action-value=\"towards-data-science\" data-collection-slug=\"towards-data-science\" href=\"https://towardsdatascience.com?source=collection_archive---------0-----------------------\">Towards Data Science</a><div class=\"ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental\"><a class=\"link link--darken\" data-action=\"open-post\" data-action-source=\"preview-listing\" data-action-value=\"https://towardsdatascience.com/graph-ml-in-2023-the-state-of-affairs-1ba920cb9232?source=collection_archive---------0-----------------------\" href=\"https://towardsdatascience.com/graph-ml-in-2023-the-state-of-affairs-1ba920cb9232?source=collection_archive---------0-----------------------\"><time datetime=\"2023-01-01T17:58:15.013Z\">Jan 1, 2023</time></a><span class=\"middotDivider u-fontSize12\"></span><span class=\"readingTime\" title=\"26 min read\"></span></div></div></div></div>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stories[0].find('div', class_='postMetaInline u-floatLeft u-sm-maxWidthFullWidth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = re.findall(r'data-action-source=\"preview-listing\" data-action-value=\"(.*?=collection_archive)', str(stories[0]))\n",
    "options = Options()\n",
    "options.add_argument(\"user-data-dir=C:/Users/znay/AppData/Local/Google/Chrome/User Data\")\n",
    "options.add_argument(r'--profile-directory=Profile 9') #e.g. Profile 3\n",
    "options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "options.add_experimental_option(\"excludeSwitches\",[\"enable logging\"])\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable automation\"])\n",
    "options.add_argument(\"start-maximized\")\n",
    "\n",
    "driver = webdriver.Chrome(service = Service(executable_path=r\"C:\\Users\\znay\\Downloads\\chromedriver.exe\"), options = options)\n",
    "\n",
    "driver.get(match[0])\n",
    "text = driver.find_element(By.XPATH, \"/html/body\").text\n",
    "\n",
    "story = BeautifulSoup(text, 'html.parser')\n",
    "\n",
    "author_box = story.find('div', class_='postMetaInline u-floatLeft u-sm-maxWidthFullWidth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Write\n",
       "STATE OF THE ART DIGEST\n",
       "Graph ML in 2023: The State of Affairs\n",
       "Hot trends and major advancements\n",
       "Michael Galkin\n",
       "·\n",
       "Follow\n",
       "Published in\n",
       "Towards Data Science\n",
       "·\n",
       "26 min read\n",
       "·\n",
       "Jan 1, 2023\n",
       "927\n",
       "3\n",
       "2022 comes to an end and it is about time to sit down and reflect upon the achievements made in Graph ML as well as to hypothesize about possible breakthroughs in 2023. Tune in 🎄☕\n",
       "Background image generated by DALL-E 2, text added by Author.\n",
       "The article is written together with Hongyu Ren (Stanford University), Zhaocheng Zhu (Mila &amp; University of Montreal). We thank Christopher Morris and Johannes Brandstetter for the feedback and helping with the Theory and PDE sections, respectively. Follow Michael, Hongyu, Zhaocheng, Christopher, and Johannes here on Medium and Twitter for more graph ml-related discussions.\n",
       "Table of Contents:\n",
       "Generative Models: Denoising Diffusion for Molecules and Proteins\n",
       "DFTs, ML Force Fields, Materials, and Weather Simulations\n",
       "Geometry &amp; Topology &amp; PDEs\n",
       "Graph Transformers\n",
       "BIG Graphs\n",
       "GNN Theory: Weisfeiler and Leman Go Places, Subgraph GNNs\n",
       "Knowledge Graphs: Inductive Reasoning Takes Over\n",
       "Algorithmic Reasoning and Alignment\n",
       "Cool GNN Applications\n",
       "Hardware: IPUs and Graphcore win OGB LSC 2022\n",
       "New Conferences: LoG and Molecular ML\n",
       "Courses and Educational Materials\n",
       "New Datasets, Benchmarks, and Challenges\n",
       "Software Libraries and Open Source\n",
       "Join the Community\n",
       "The Meme of 2022\n",
       "Generative Models: Denoising Diffusion for Molecules and Proteins\n",
       "Generative diffusion models in the vision-language domain were the headline topic in the Deep Learning world in 2022. While generating images and videos is definitely a cool playground to try out different models and sampling techniques, we’d argue that\n",
       "the most useful applications of diffusion models in 2022 were actually created in the Geometric Deep Learning area focusing on molecules and proteins\n",
       "In our recent article, we were pondering whether “Denoising Diffusion Is All You Need?”.\n",
       "Denoising Diffusion Generative Models in Graph ML\n",
       "Is Denoising Diffusion all you need?\n",
       "towardsdatascience.com\n",
       "There, we reviewed newest generative models for graph generation (DiGress), molecular conformer generation (EDM, GeoDiff, Torsional Diffusion), molecular docking (DiffDock), molecular linking (DiffLinker), and ligand generation (DiffSBDD). As soon as the post went public, several amazing protein generation models were released:\n",
       "Chroma from Generate Biomedicines allows to impose functional and geometric constraints, and even use natural language queries like “Generate a protein with CHAD domain” thanks to a small GPT-Neo trained on protein captioning;\n",
       "Chroma protein generation. Source: Generate Biomedicines\n",
       "RoseTTaFold Diffusion (RF Diffusion) from the Baker Lab and MIT is packed with the similar functionality also allowing for text prompts like “Generate a protein that binds to X” as well as being capable of functional motif scaffolding, scaffolding enzyme active sites, and de novo protein design. Strong point: 1000 designs generated with RF Diffusion were experimentally synthesized and tested in the lab!\n",
       "RF Diffusion. Source: Watson et al. BakerLab\n",
       "The Meta AI FAIR team made amazing progress in protein design purely with language models: mid-2022, ESM-2 was released, a protein LM trained solely on protein sequences that outperforms ESM-1 and other baselines by a huge margin. Moreover, it was then shown that encoded LM representations are a very good starting point for obtaining the actual geometric configuration of a protein without the need for Multiple Sequence Alignments (MSAs) — this is done via ESMFold. A big shoutout to Meta AI and FAIR for publishing the model and the weights: it is available in the official GitHub repo and on HuggingFace as well!\n",
       "Scaling ESM-2 leads to better folding prediction. Source: Lin, Akin, Rao, Hie et al\n",
       "🍭 Later on, even more goodies arrived from the ESM team: Verkuil et al. find that ESM-2 can generate de novo protein sequences that can actually be synthesized in the lab and, more importantly, do not have any match among known natural proteins. Hie et al. propose pretty much a new programming language for protein designers (think of it as a query language for ESMFold) — production rules organized in a syntax tree with constraint functions. Then, each program is “compiled” into an energy function that governs the generative process. Meta AI also released the biggest Metagenomic Atlas, but more on that in the Datasets section of this article.\n",
       "In the antibody design area, a similar LM-based approach is taken by IgLM by Shuai, Ruffolo, and Gray. IGLM generates antibody sequences conditioned on chain and species id tags.\n",
       "Finally, we’d highlight a few works from Jian Tang’s lab at Mila. MoleculeSTM by Liu et al. is a CLIP-like text-to-molecule model (plus a new large pre-training dataset). MoleculeSTM can do 2 impressive things: (1) retrieve molecules by text description like “triazole derivatives” and retrieve text description from a given molecule in SMILES, (2) molecule editing from text prompts like “make the molecule soluble in water with low permeability” — and the model edits the molecular graph according to the description, mindblowing 🤯\n",
       "Then, ProtSEED by Shi et al. is a generative model for protein sequence and structure simultaneously (for example, most existing diffusion models for proteins can do only one of those at a time). ProtSEED can be conditioned on residue features or pairs of residues. Model-wise, it is an equivariant iterative model with improved triangular attention. ProtSEED was evaluated on Antibody CDR co-design, Protein sequence-structure co-design, and Fixed backbone sequence design.\n",
       "Molecule editing from text inputs. Source: Liu et al.\n",
       "Besides generating the protein structures, there are also some works for generating protein sequences from structures, known as inverse folding. Don’t forget to check out the ESM-IF1 from Meta and the ProteinMPNN from the Baker Lab.\n",
       "What to expect in 2023: (1) performance improvements of diffusion models such as faster sampling and more efficient solvers; (2) more powerful conditional protein generation models; (3) more successful applications of Generative Flow Networks (GFlowNets, check out the tutorial) to molecules and proteins.\n",
       "DFTs, ML Force Fields, Materials, and Weather Simulations\n",
       "AI4Science becomes the frontier of equivariant GNN research and its applications. Pairing GNNs with PDEs, we can now tackle much more complex prediction tasks.\n",
       "In 2022, this frontier expanded to ML-based Density Functional Theory (DFT) and Force fields approximations used for molecular dynamics and material discovery. The other growing field is Weather simulations.\n",
       "We would recommend the talk by Max Welling for a broader overview of AI4Science and what is now enabled by using Deep Learning in science.\n",
       "Starting with models, 2022 has seen a surge in equivariant GNNs for molecular dynamics and simulations, e.g., building upon NequIP, Allegro by Musaelian, Batzner, et al. or MACE by Batatia et al. The design space for such models is very large, so refer to the recent survey by Batatia, Batzner, et al. for an overview. A crucial component for most of them is the e3nn library (paper by Geiger and Smidt) and the notion of tensor product. We highly recommend a great new course by Erik Bekkers on Group Equivariant Deep Learning to understand the mathematical foundations and catch up with the recent papers.\n",
       "⚛️ Density Functional Theory (DFT) calculations are one of the main workhorses of molecular dynamics (and account for a great deal of computing time in big clusters). DFT is O(n³) to the input size though, so can ML help here? In Learned Force Fields Are Ready For Ground State Catalyst Discovery, Schaarschmidt et al. present the experimental study of models of learned potentials — turns out GNNs can do a very good job in linear O(n) time! The Easy Potentials approach (trained on Open Catalyst data) turns out to be quite a good predictor especially when paired with a postprocessing step. Model-wise, it is an MPNN with the Noisy Nodes self-supervised objective.\n",
       "In Forces are not Enough, Fu et al. introduce a new benchmark for molecular dynamics — in addition to MD17, the authors add datasets on modeling liquids (Water), peptides (Alanine dipeptide), and solid-state materials (LiPS). More importantly, the authors consider a wide range of physical properties like stability of simulations, diffusivity, and radial distribution functions. Most SOTA molecular dynamics models were probed including SchNet, ForceNet, DimeNet, GemNet (-T and -dT), and NequIP.\n",
       "Source: Fu et al.\n",
       "In crystal structure modeling, we’d highlight Equivariant Crystal Networks by Kaba and Ravanbakhsh — a neat way to build representations of periodic structures with crystalline symmetries. Crystals can be described with lattices and unit cells with basis vectors that are subject to group transformations. Conceptually, ECN creates edge index masks corresponding to symmetry groups, performs message passing over this masked index, and aggregates the results of many symmetry groups.\n",
       "Source: Kaba and Ravanbakhsh\n",
       "Even more news on material discovery can found in the proceedings of the recent AI4Mat NeurIPS workshop!\n",
       "☂️ ML-based weather forecasting made a huge progress as well. In particular, GraphCast by DeepMind and Pangu-Weather by Huawei demonstrated exceptionally good results outperforming traditional models by a large margin. While Pangu-Weather leverages 3D/visual inputs and Visual Transformers, GraphCast employs a mesh MPNN where Earth is split into several hierarchy levels of meshes. The deepest level has about 40K nodes with 474 input features and the model outputs 227 predicted variables. The MPNN follows the “encoder-processor-decoder” and has 16 layers. GraphCast is autoregressive model w.r.t. the next timestep prediction, that is, it takes previous two states and predicts the next one. GraphCast can build a 10-day forecast in &lt;60 seconds on a single TPUv4 and is much more accurate than non-ML forecasting models. 👏\n",
       "Encoder-Processor-Decoder mesh MPNN in GraphCast. Source: Lam, Sanchez-Gonzalez, Willson, Wirnsberger, Fortunato, Pritzel, et al.\n",
       "What to expect in 2023: We expect to see a lot more focus on computational efficiency and scalability of GNNs. Current GNN-based force-fields are obtaining remarkable accuracy, but are still 2–3 orders of magnitude slower than classical force-fields and are typically only deployed on a few hundred atoms. For GNNs to truly have a transformative impact on materials science and drug discovery, we will see many folks tackling this issue, be it through architectural advances or smarter sampling.\n",
       "Geometry &amp; Topology &amp; PDEs\n",
       "In 2022, 1️⃣ we got a better understanding of oversmoothing and oversquashing phenomena in GNNs and their connections to algebraic topology; 2️⃣ using GNNs for PDE modeling is now mainstream.\n",
       "1️⃣ Michael Bronstein’s lab made huge contributions to this problem — check those excellent posts on Neural Sheaf Diffusion and framing GNNs as gradient flows\n",
       "Neural Sheaf Diffusion for deep learning on graphs\n",
       "Cellular sheaf theory, a branch of algebraic topology, provides new insights into how Graph Neural Networks work and…\n",
       "towardsdatascience.com\n",
       "And on GNNs as gradient flows:\n",
       "Graph Neural Networks as gradient flows\n",
       "GNNs derived as gradient flows minimising a learnable energy that describes attractive and repulsive forces between…\n",
       "towardsdatascience.com\n",
       "2️⃣ Using GNNs for PDE modeling became a mainstream topic. Some papers require the 🤯 math alert 🤯 warning, but if you are familiar with the basics of ODEs and PDEs it should be much easier.\n",
       "Message Passing Neural PDE Solvers by Brandstetter, Worrall, and Welling describe how message passing can help solving PDEs, generalize better, and get rid of manual heuristics. Furthermore, MP-PDEs representationally contain classic solvers like finite differences.\n",
       "Source: Brandstetter, Worrall, and Welling\n",
       "The topic was developed further by many recent works including continuous forecasting with implicit neural representations (Yin et al.), supporting mixed boundary conditions (Horie and Mitsume), or latent evolution of PDEs (Wu et al.)\n",
       "What to expect in 2023: Neural PDEs and their applications are likely to expand to more physics-related AI4Science subfields, where especially computational fluid dynamics (CFD) will potentially be influenced by GNN based surrogates in the coming months. Classical CFD is applied to a wide range of research and engineering problems in many fields of study, including aerodynamics, hypersonic and environmental engineering, fluid flows, visual effects in video games, or weather simulations as discussed above. GNN based surrogates might augment/replace traditional well-tried techniques such as finite element methods (Lienen et al.), remeshing algorithms (Song et al.), boundary value problems (Loetsch et al.), or interactions with triangularized boundary geometries (Mayr et al.).\n",
       "The neural PDE community is starting to build strong and commonly used baselines and frameworks, which will in return help to accelerate the progress, e.g. PDEBench (Takamoto et al.) or PDEArena (Gupta et al.)\n",
       "Graph Transformers\n",
       "Definitely one of the main community drivers in 2022, graph transformers (GTs) evolved a lot towards higher effectiveness and better scalability. Several outstanding models published in 2022:\n",
       "👑 GraphGPS by Rampášek et al. takes the title of “GT of 2022” thanks to combining local message passing, global attention (optionally, linear for higher efficiency), and positional encodings that led to setting a new SOTA on ZINC and many other benchmarks. Check out a dedicated article on GraphGPS\n",
       "GraphGPS: Navigating Graph Transformers\n",
       "Recipes for cooking the best graph transformers\n",
       "towardsdatascience.com\n",
       "GraphGPS served as a backbone of GPS++, the winning OGB Large Scale Challenge 2022 model on PCQM4M v2 (graph regression). GPS++, created by Graphcore, Valence Discovery, and Mila, incorporates more features including 3D coordinates and leverages sparse-optimized IPU hardware (more on that in the following section). GPS++ weights are already available on GitHub!\n",
       "GraphGPS intuition. Source: Rampášek et al\n",
       "Transformer-M by Luo et al. inspired many top OGB LSC models as well. Transformer-M adds 3D coordinates to the neat mix of joint 2D-3D pre-training. At inference time, when 3D info is not known, the model would infer a glimpse of 3D knowledge which improves the performance on PCQM4Mv2 by a good margin. Code is available either.\n",
       "Transformer-M joint 2D-3D pre-training scheme. Source: Luo et al.\n",
       "TokenGT by Kim et al goes even more explicit and adds all edges of the input graph (in addition to all nodes) to the sequence fed to the Transformer. With those inputs, encoder needs additional token types to distinguish nodes from edges. The authors prove several nice theoretical properties (although at the cost of higher computational complexity O((V+E)²) that can get to the 4th power in the worst case of a fully-connected graph). Code is available.\n",
       "TokenGT adds both nodes and edges to the input sequence. Source: Kim et al\n",
       "What to expect in 2023: for the coming year, we’d expect 1️⃣ GTs to scale up along the axes of both data and model parameters, from molecules of &lt;50 nodes to graphs of millions of nodes, in order to witness the emergent behavior as in text &amp; vision foundation models 2️⃣ similar to BLOOM by the BigScience Initiative, a big open-source pre-trained equivariant GT for molecular data, perhaps within the Open Drug Discovery project.\n",
       "BIG Graphs\n",
       "🔥 One of our favorites in 2022 is “Graph Neural Networks for Link Prediction with Subgraph Sketching” by Chamberlain, Shirobokov et al. — this is a neat combination of algorithms + ML techniques. It is known that SEAL-like labeling tricks dramatically improve link prediction performance compared to standard GNN encoders but suffer from big computation/memory overhead. In this work, the authors find that obtaining distances from two nodes of a query edge can be efficiently done with hashing (MinHashing) and cardinality estimation (HyperLogLog) algorithms. Essentially, message passing is done over minhashing and hyperloglog initial sketches of single nodes (min aggregation for minhash, max for hyperloglog sketches) — this is the core of the ELPH link prediction model (with a simple MLP decoder). The authors then design a more scalable BUDDY model where k-hop hash propagation can be precomputed before training. Experimentally, ELPH and BUDDY scale to large graphs that were previously way too large or resource hungry for labeling trick approaches. Great work and definitely a solid baseline for all future link prediction models! 👏\n",
       "The motivation behind computing subgraph hashes to estimate cardinalities of neighborhoods and intersections. Source: Chamberlain, Shirobokov et al.\n",
       "On the graph sampling and minibatching side, Gasteiger, Qian, and Günnemann design Influence-based Mini-Batching (IBMB), a good example how Personalized PageRank (PPR) can solve even graph batching! IBMB aims at creating the smallest minibatches whose nodes have the maximum influence on the node classification task. In fact, the influence score is equivalent to PPR. Practically, given a set of target nodes, IBMB (1) partitions the graph into permanent clusters, (2) runs PPR within each batch to select top-PPR nodes that would form a final subgraph minibatch. The resulting minibatches can be sent to any GNN encoder. IBMB is pretty much constant O(1) to the graph size where partitioning and PPRs can be precomputed at the pre-processing stage.\n",
       "Although the resulting batches are fixed and do not change over training (not stochastic enough), the authors design momentum-like optimization terms to mitigate this non-stochasticity. IBMB can be used both in training and inference with massive speedups — up to 17x and 130x, respectively 🚀\n",
       "Influence-based mini-batching. Source: Gasteiger, Qian, and Günnemann\n",
       "The subtitle of this subsection could be “brought to you by Google” since the majority of the papers have authors from Google ;)\n",
       "Carey et al. created Stars, a method for building sparse similarity graphs at the scale of tens of trillions of edges 🤯. Pairwise N² comparisons would obviously not work here — Stars employs two-hop spanner graphs (those are the graphs where similar points are connected with at most two hops) and SortingLSH that together enable almost linear time complexity and high sparsity.\n",
       "Dhulipala et al. created ParHAC, an approximate (1+𝝐) parallel algorithm for hierarchical agglomerative clustering (HAC) on very large graphs and extensive theoretical foundations of the algorithm. ParHAC has O(V+E) complexity and poly-log depth and runs up to 60x faster than baselines on graphs with hundreds of billions of edges (here it is the Hyperlink graph with 1.7B nodes and 125B edges).\n",
       "Devvrit et al. created S³GC, a scalable self-supervised graph clustering algorithm with one-layer GNN and constrastive training objective. S³GC uses both graph structure and node features and scales to graphs of up to 1.6B edges.\n",
       "Finally, Epasto et al. created a differentially-private modification of PageRank!\n",
       "LoG 2022 featured two tutorials on large-scale GNNs: Scaling GNNs in Production by Da Zheng, Vassilis N. Ioannidis, and Soji Adeshina and Parallel and Distributed GNNs by Torsten Hoefler and Maciej Besta.\n",
       "What to expect in 2023: further reduction in compute costs and inference time for very large graphs. Perhaps models for OGB LSC graphs could run on commodity machines instead of huge clusters?\n",
       "GNN Theory: Weisfeiler and Leman Go Places, Subgraph GNNs\n",
       "Tourists of the year! Source of the original portraits: Towards Geometric Deep Learning IV: Chemical Precursors of GNNs by Michael Bronstein\n",
       "🏖 🌄 Weisfeiler and Leman, grandfathers of Graph ML and GNN theory, had a very prolific traveling year! After visiting Neural, Sparse, Topological, and Cellular places in previous years, in 2022 we have seen them in several new places:\n",
       "WL Go Machine Learning — a comprehensive survey by Morris et al on the basics of the WL test, terminology, and various applications;\n",
       "WL Go Relational — the first attempt by Barcelo et al to study expressiveness of relational GNNs used in multi-relational graphs and KGs. Turns out R-GCN and CompGCN are equally expressive and are bounded by the Relational 1-WL test, and the most expressive message function (aggregating entity-relation representations) is a Hadamard product;\n",
       "WL Go Walking by Niels M. Kriege studies expressiveness of random walk kernels and finds that the RW kernel (with a small modification) is as expressive as a WL subtree kernel;\n",
       "WL Go Geometric: Joshi, Bodnar et al propose Geometric WL test (GWL) to study expressiveness of equivariant and invariant GNNs (to ceratin symmetries: translation, rotation, reflection, permutation). Turns out, equivariant GNNs (such as E-GNN, NequIP or MACE) are provably more powerful than invariant GNNs (such as SchNet or DimeNet);\n",
       "WL Go Temporal: Souza et al propose Temporal WL test to study expressiveness of temporal GNNs. The authors then propose a novel injective aggregation function (and the PINT model) that should be most expressive;\n",
       "WL Go Gradual: Bause and Kriege propose to modify the original WL color refinement with a non-injective function where different multi-sets might get assigned the same color (under certain conditions). It thus enables more gradual color refinement and slower convergence to stable coloring that eventually retains expressiveness of 1-WL but gets a few distinguishing properties on the way.\n",
       "WL Go Infinite: Feldman et al propose to change the initial node coloring with spectral features derived from the heat kernel of the Laplacian or with k-smallest eigenvectors of the Laplacian (for large graphs) which is quite close to Laplacian Positional Encodings (LPEs).\n",
       "WL Go Hyperbolic: Nikolentzos et al note that the color refinement procedure of the WL test produces a tree hierarchy of colors. In order to preserve relative distances of nodes encoded by those colors, the authors propose to map output states of each layer/iteration into a hyperbolic space and update it after each next layer. The final embeddings are supposed to retain the notion of node distances.\n",
       "📈 In the realm of more expressive (than 1-WL) architectures, subgraph GNNs are the biggest trend. Among those, three approaches stand out: 1️⃣ Subgraph Union Networks (SUN) by Frasca, Bevilacqua, et al. that provide a comprehensive analysis of subgraph GNNs design space and expressiveness showing they are bounded by 3-WL; 2️⃣ Ordered Subgraph Aggregation Networks (OSAN) by Qian, Rattan, et al devise a hierarchy of subgraph-enhanced GNNs (k-OSAN) and find that k-OSAN are incomparable to k-WL but are strictly limited by (k+1)-WL. One particularly cool part of OSAN is using Implicit MLE (NeurIPS’21), a differentiable discrete sampling technique, for sampling ordered subgraphs. ️3️⃣ SpeqNets by Morris et al. devise a permutation-equivariant hierarchy of graph networks that balances between scalability and expressivity. 4️⃣ GraphSNN by Wijesinghe and Wang derives expressive models based on the overlap of subgraph isomorphisms and subtree isomorpishms.\n",
       "🤔 A few works rethink the WL framework as a general means for GNN expressiveness. Geerts and Reutter define k-order MPNNs that can be characterized with Tensor Languages (with a mapping between WL and Tensor Languages). A new anonymous ICLR’23 submission proposes to leverage graph biconnectivity and defines a Generalized Distance WL algorithm.\n",
       "If you’d like to study the topic even deeper, check out a wonderful LOG 2022 tutorial by Fabrizio Frasca, Beatrice Bevilacqua, and Haggai Maron with practical examples!\n",
       "What to expect in 2023: 1️⃣ More efforts on creating time- and memory-efficient subgraph GNNs. 2️⃣ Better understanding of generalization of GNNs. 3️⃣ Weisfeiler and Leman visit 10 new places!\n",
       "Knowledge Graphs: Inductive Reasoning Takes Over\n",
       "Last year, we observed a major shift in KG representation learning: transductive-only approaches are being actively retired in favor of inductive models that can build meaningful representation for new, unseen nodes and perform node classification and link prediction.\n",
       "In 2022, the field was expanding along two main axes: 1️⃣ inductive link prediction (LP) 2️⃣ and inductive (multi-hop) query answering that extends link prediction to much more complex prediction tasks.\n",
       "1️⃣ In link prediction, the majority of inductive models (like NBFNet or NodePiece) transfer to unseen nodes at inference by assuming that the set of relation types is fixed during training and does not change over time so they can learn relation embeddings. What happens when the set of relations changes as well? In the hardest case, we’d want to transfer to KGs with completely different nodes and relation types.\n",
       "So far, all such models supporting unseen relations resort to meta-learning which is slow and resource-hungry. In 2022, for the first time, Huang, Ren, and Leskovec proposed the Connected Subgraph Reasoner (CSR) framework that is inductive along both entities and relation types and does not need any meta-learning! 👀 Generally, for new relations at inference, models see at least k example triples with this relation (hence, a k-shot learning scenario). Conceptually, CSR extracts subgraphs around each example trying to learn common relational patterns (i.e., optimizing edge masks) and then apply the mask to the query subgraph (with the missing target link to predict).\n",
       "Inductive CSR that supports KGs with unseen entities and relation types. Source: Huang, Ren, and Leskovec\n",
       "ReFactor GNNs by Chen et al. is another insightful work on inductive qualities of shallow KG embedding models — particularly, the authors find that shallow factorization models like DistMult resemble infinitely deep GNNs when looking through the lens of backpropagation and how nodes update their representations from neighboring and non-neighboring nodes. Turns out that, theoretically, any factorization model can be turned into an inductive model!\n",
       "2️⃣ Inductive representation learning arrived in the area of complex logical query answering as well. (shameless plug) In fact, it was one of the focuses of our team this year 😊 First, in Zhu et al., we found that Neural Bellman-Ford nets generalize well from simple link prediction to complex query answering tasks in a new GNN Query Executor (GNN-QE) model where a GNN based on NBF-Net performs relation projections while other logical operators are performed via fuzzy logic t-norms. Then, in Inductive Logical Query Answering in Knowledge Graphs we studied ⚗️ the essence of inductiveness ⚗️ and proposed two ways to answer logical queries over unseen entities at inference time, that is, via (1) inductive node representations obtained with NodePiece encoder paired with the inference-only decoder (less performant but scalable) or via (2) inductive relational structure representations akin to the one in GNN-QE (better quality but more resource-hungry and hard to scale). Overall we are able to scale to an inductive query setting on graphs with millions of nodes and 500k unseen nodes and 5m unseen edges during inference.\n",
       "Inductive logical query answering approaches: via node representations (NodePiece-QE) and relational structure representations (GNN-QE). Source: Galkin et al.\n",
       "The other cool work in the area is SMORE by Ren, Dai, et al. — it is a large-scale (transductive-only yet) system for complex query answering over very large graphs scaling up to the full Freebase with about 90M nodes and 300M edges 👀. In addition to CUDA, training, and pipeline optimizations, SMORE implements a bidirectional query sampler such that training queries can be generated on-the-fly right in the data loader instead of creating and storing huge datasets. Don’t forget to check out a fresh hands-on tutorial on large-scale graph reasoning from LOG 2022!\n",
       "Last but not the least, Yang, Lin and Zhang brought up an interesting paper rethinking the evaluation of knowledge graph completion. They point out knowledge graphs tend to be open-world (i.e., there are facts not encoded by the knowledge graph) rather close-world assumed by most works. As a result, metrics observed under the close-world assumption exhibit a log trend w.r.t. the true metric — this means if you get 0.4 MRR for your model, chances are that the test knowledge graph is incomplete and your model has already done a good job👍. Maybe we can design some new dataset and evaluation to mitigate such an issue?\n",
       "What to expect in 2023: an inductive model fully transferable to different KGs with new sets of entities and relations, e.g., training on Wikidata, and running inference on DBpedia or Freebase.\n",
       "Algorithmic Reasoning and Alignment\n",
       "2022 was a year of major breakthroughs and milestones for algorithmic reasoning.\n",
       "1️⃣ First, the CLRS benchmark by Veličković et al. is now available as the main playground to design and benchmark algorithmic reasoning models and tasks. CLRS already includes 30 tasks (such as classical sorting algorithms, string algorithms, and graph algorithms) but still allows you to bring your own formulations or modify existing ones.\n",
       "2️⃣ Then, a Generalist Neural Algorithmic Learner by Ibarz et al. and DeepMind has shown that it is possible to train a single processor network that can be trained in the multi-task mode on different algorithms — previously, you’d train a single model for a single task repeating that for all 30 CLRS problems. The paper also describes several modifications and tricks to the model architecture side and training procedure to let the model generalize better and prevent forgetting, e.g., triplet reasoning similar to triangular attention (common for molecular models) and edge transformers. Overall, a new model brings a massive 25% absolute gain over baselines and solves 24 out of 30 CLRS tasks with 60%+ micro-F1.\n",
       "Source: Ibarz et al.\n",
       "3️⃣ Last year, we discussed the works on algorithmic alignment and saw the signs that GNNs can probably align well with dynamic programming. In 2022, Dudzik and Veličković prove that GNNs are Dynamic Programmers using category theory, abstract algebra, and notion of pushforward and pullback operations. This is a wonderful example of applying category theory that many people consider “abstract nonsense” 😉. Category theory is likely to have more impact in GNN theory and Graph ML in general, so check out a fresh course Cats4AI for a gentle introduction to the field.\n",
       "4️⃣ Finally, the work of Beurer-Kellner et al. is one of the first practical application of the neural algorithmic reasoning framework, here it is applied to configuring computer networks, i.e., routing protocols like BGP that are at the core of the internet. There, the authors show that representing a routing config as a graph allows to frame the routing problem as node property prediction. This approach brings whopping 👀 490x 👀 speedups compared to traditional rule-based routing methods and stil maintain 90+% specification consistency.\n",
       "Source: Beurer-Kellner et al.\n",
       "If you want to follow algorithmic reasoning more closely, don’t miss a fresh LoG 2022 tutorial by Petar Veličković, Andreea Deac and Andrew Dudzik.\n",
       "What to expect in 2023: 1️⃣ Algorithmic reasoning tasks are likely to scale to graphs of thousands of nodes and practical applications like in code analysis or databases, 2️⃣ even more algorithms in the benchmark, 3️⃣ most unlikely — there will appear a model capable of solving quickselect 😅\n",
       "Cool GNN Applications\n",
       "👃 Learning to Smell with GNNs. Back in 2019, Google AI started a project on learning representations of smells. From basic chemistry we know that aromaticity depends on the molecular structure, e.g., cyclic compounds. In fact, the whole group of ”aromatic hydrocarbons” was named aromatic because they actually has some smell (compared to many non-organic molecules). If we have a molecular structure, we can employ a GNN on top of it and learn some representations!\n",
       "Recently, Google AI released a new blogpost and paper by Qian et al. describing the next phase of the project — the Principal Odor Map that is able to group molecules in “odor clusters”. The authors conducted 3 cool experiments: classifying 400 new molecules never smelled before and comparison to the averaged rating of a group of human panelists; linking odor quality to fundamental biology; and probing aromatic molecules on their mosquito repelling qualities. The GNN-based model shows very good results — now we can finally claim that GNNs can smell! Looking forward for GNNs transforming the perfume industry.\n",
       "Embedding of odors. Source: Google AI blog\n",
       "⚽ GNNs + Football. If you thought that sophisticated GNNs for modelling trajectories are only used for molecular dynamics and arcane quantum simulations, fear not! Here is a cool practical application with a very high potential outreach: Graph Imputer by Omidshafiei et al., DeepMind, and FC Liverpool predicts trajectories of football players (and the ball). Each game graph consists of 23 nodes, gets updated with a standard message passing encoder and a special time-dependent LSTM. The dataset is quite novel, too — it consists of 105 English Premier League matches (avg 90 min each), all players and the ball were tracked at 25 fps, and the resulting training trajectory sequences encode about 9.6 seconds of gameplay.\n",
       "The paper is easy to read and has numerous football illustrations, check it out! Sports tech is actively growing those days, and football analysts now could go even deeper in studying their competitors. Will EPL clubs compete for GNN researchers in the upcoming transfer windows? Time to create transfermarkt for GNN researchers 😉\n",
       "Football match simulation is like molecular dynamics simulation! Source: DeepMind\n",
       "🪐 Galaxies and Astrophysics. For astrophysics aficionados: Mangrove by Jespersen et al. applies GraphSAGE to merger trees of dark matter to predict a variety of galactic properties like stellar mass, cold gas mass, star formation rate, and even black hole mass. The paper is a bit heavy on the terminology of astrophysics but pretty easy in terms of GNN parameterization and training. Mangrove works 4–9 orders of magnitude faster than standard models. Experimental charts are pieces of art that you can hang on a wall 🖼️.\n",
       "Mangrove approach to present dark matter halos as merger trees and graphs. Source: Jespersen et al.\n",
       "🤖 GNNs for code. Code generation models like AlphaCode and Codex have mindblowing capabilities. Although LLMs are at the core of those models, GNNs do help in a few neat ways: Instruction Pointer Attention GNNs (IPA-GNNs) first proposed by Bieber et al have been used to predict runtime errors in competitive programming tasks — so it is almost like a virtual code interpreter! CodeTrek by Pashakhanloo et al. proposes to model a program as a relational graph and embed it via random walks and Transformer encoder. Downstream applications include variable misuse, prediction exceptions, predicting shadowed variables.\n",
       "Source: Pashakhanloo et al.\n",
       "Hardware: IPUs and Graphcore Win OGB Large-Scale Challenge 2022\n",
       "🥇 2022 brought a huge success to Graphcore and IPUs — the hardware optimized for sparse operations that are so needed when working with graphs. The first success story was optimizing Temporal Graph Nets (TGN) for IPUs with massive performance gains (check the article in Michael Bronstein’s blog).\n",
       "Accelerating and scaling Temporal Graph Networks on the Graphcore IPU\n",
       "Is GPU the best hardware choice for GNNs? Together with Graphcore, we explore the advantages of the new IPU…\n",
       "towardsdatascience.com\n",
       "Later on, Graphcore stormed the leaderboards of OGB LSC’22 by winning 2 out of 3 tracks: link prediction on the WikiKG90M v2 knowledge graph and graph regression on the PCQM4M v2 molecular dataset. In addition to the sheer compute power, the authors took several clever model decisions: for link prediction it was Balanced Entity Sampling and Sharing (BESS) for training an ensemble of shallow LP models (check the blog post by Daniel Justus for more details), and GPS++ for the graph regression task (we covered GPS++ above in the GT section). You can try out the pre-trained models using IPUs-powered virtual machines on Paperspace. Congratulations to Graphcore and their team! 👏\n",
       "PyG partnered with NVIDIA (post) and Intel (post) to increase the performance of core operations on GPUs and CPUs, respectively. Similarly, DGL incorporated new GPU optimizations in the recent 0.9 version. Massive gains for sparse matmuls and sampling procedures, so we’d encourage you to update your environments with the most recent versions!.\n",
       "What to expect in 2023: major GNN libraries are likely to increase the breadth of supported hardware backends such as IPUs or upcoming Intel Max Series GPUs.\n",
       "New Conferences: Learning of Graphs (LoG) and Molecular ML (MoML)\n",
       "This year we witnessed the inauguration of two graph and geometric ML conferences: the Learning on Graphs Conference (LoG) and the Molecular ML Conference (MoML).\n",
       "LoG is a more general all-around GraphML venue (held virtually this year) while MoML (held at MIT) has a broader mission and influence over the AI4Science community where graphs and geometry still plays a major role. Both conferences were received extremely well. MoML attracted 7 top speakers and 38 posters, LoG had ~3000 registrations, 266 submissions, 71 posters, 12 orals, and 7 awesome tutorials (all recordings of oral talks and tutorials are already on YouTube). Besides, LoG introduced a great monetary incentive for reviewers, resulting in a well-recognized improvement of the review quality! From our point of view, quality of LoG reviews was often better than those at NeurIPS or ICML.\n",
       "This is a huge win and carnival for the graph ML community, and congrats to everyone working in the field of graph and geometric machine learning with a new “home” venue!\n",
       "What to expect in 2023: LOG and MoML become main Graph ML venues to include into your submission calendar along with ICLR / NeurIPS / ICML\n",
       "Courses and Educational Materials\n",
       "Geometric Deep Learning Course — Second Edition (2022) is already on YouTube. The main entry point to the field.\n",
       "An Introduction to Group Equivariant Deep Learning by Erik Bekkers — one of the best new courses about equivariance and equivariant models!\n",
       "Cats4AI — a new course by Andrew Dudzik, Bruno Gavranović, João Guilherme Araújo, Petar Veličković, and Pim de Haan is the best place to learn about category theory and its connections to Geometric DL.\n",
       "Summer School proceedings: Italian Summer School on Geometric DL, London Geometry and Machine Learning (LOGML) Summer School, BIRS Workshop on Topological Representation Learning.\n",
       "Stanford Graph Learning Workshop 2022 — latest news from PyG developers and partners and Stanford researchers.\n",
       "New Datasets, Benchmarks, and Challenges\n",
       "OGB Large-Scale Challenge 2022: The second large scale challenge held at NeurIPS2022 with large and realistic graph ML tasks covering node-, edge-, graph-level predictions.\n",
       "Open Catalyst 2022 Challenge: the second edition of the challenge held at NeurIPS2022 with the task to design new machine learning models to predict the outcome of catalyst simulations used to understand activity\n",
       "CASP 15: the protein structure prediction challenge disrupted by AlphaFold a few years ago at CASP 14. Detailed analysis is yet to come, but it seems that MSAs strike back and best performing models still rely on MSAs.\n",
       "Long Range Graph Benchmark: for measuring GNNs and GTs capabilities of capturing long range interactions in graphs.\n",
       "Taxonomy of Graph Benchmarks, Graph Learning Indexer: deeper studies of the dataset landscape in Graph ML outlining open challenges in benchmarking and trustworthiness of results.\n",
       "GraphWorld: a framework for analyzing the performance of GNN architectures on millions of synthetic benchmark datasets\n",
       "Chartalist — a collection of blockchain graph datasets\n",
       "PEER protein learning benchmark: a multi-task benchmark for protein sequence understanding with 17 tasks of protein understanding lying in 5 task categories.\n",
       "ESM Metagenomic Atlas: acomprehensive database of over 600 million predicted protein structures with nice visualizations and search UI.\n",
       "Software Libraries and Open Source\n",
       "Mainstream graph ML libraries: PyG 2.2 (PyTorch), DGL 0.9 (PyTorch, TensorFlow, MXNet), TF GNN (TensorFlow) and Jraph (Jax)\n",
       "TorchDrug and TorchProtein: machine learning library for drug discovery and protein science\n",
       "PyKEEN: the best platform for training and evaluating knowledge graph embeddings\n",
       "Graphein: a package that provides a number of types of graph-based representations of proteins\n",
       "GRAPE and Marius: scalable graph processing and embedding libraries over billion-scale graphs\n",
       "MatSci ML Toolkit: a flexible framework for deep learning on the opencatalyst dataset\n",
       "E3nn: the go-to library for E(3) equivariant neural networks\n",
       "Join the Community\n",
       "Reading Groups: Learning on Graphs and Geometry (LOG2) reading group, Molecular Modeling &amp; Drug Discovery (M2D2) reading group, and their Slack communities\n",
       "Learning of Graphs (LoG) Slack community\n",
       "Michael Bronstein’s blog on Medium\n",
       "PyG medium, blog posts, and newsletter\n",
       "GraphML Telegram channel\n",
       "The Meme of 2022 🪓\n",
       "Created by Michael Galkin and Michael Bronstein\n",
       "Artificial Intelligence\n",
       "Machine Learning\n",
       "Deep Learning\n",
       "Graph Machine Learning\n",
       "Editors Pick\n",
       "927\n",
       "3\n",
       "Written by Michael Galkin\n",
       "3.5K Followers\n",
       "·\n",
       "Writer for \n",
       "Towards Data Science\n",
       "AI Research Scientist @ Intel Labs. Working on Graph ML, Geometric DL, and Knowledge Graphs\n",
       "Follow\n",
       "More from Michael Galkin and Towards Data Science\n",
       "Michael Galkin\n",
       "in\n",
       "Towards Data Science\n",
       "Graph &amp; Geometric ML in 2024: Where We Are and What’s Next (Part I — Theory &amp; Architectures)\n",
       "Trends and recent advancements in Graph and Geometric Deep Learning\n",
       "30 min read\n",
       "·\n",
       "Jan 15, 2024\n",
       "--\n",
       "2\n",
       "Dave Melillo\n",
       "in\n",
       "Towards Data Science\n",
       "Building a Data Platform in 2024\n",
       "How to build a modern, scalable data platform to power your analytics and data science projects (updated)\n",
       "9 min read\n",
       "·\n",
       "Feb 5, 2024\n",
       "--\n",
       "25\n",
       "Leonie Monigatti\n",
       "in\n",
       "Towards Data Science\n",
       "Intro to DSPy: Goodbye Prompting, Hello Programming!\n",
       "How the DSPy framework solves the fragility problem in LLM-based applications by replacing prompting with programming and compiling\n",
       "·\n",
       "13 min read\n",
       "·\n",
       "Feb 27, 2024\n",
       "--\n",
       "8\n",
       "Michael Galkin\n",
       "in\n",
       "Towards Data Science\n",
       "Graph &amp; Geometric ML in 2024: Where We Are and What’s Next (Part II — Applications)\n",
       "Trends and recent advancements in Graph and Geometric Deep Learning\n",
       "42 min read\n",
       "·\n",
       "Jan 16, 2024\n",
       "--\n",
       "5\n",
       "See all from Michael Galkin\n",
       "See all from Towards Data Science\n",
       "Recommended from Medium\n",
       "Michael Galkin\n",
       "in\n",
       "Towards Data Science\n",
       "Graph &amp; Geometric ML in 2024: Where We Are and What’s Next (Part II — Applications)\n",
       "Trends and recent advancements in Graph and Geometric Deep Learning\n",
       "42 min read\n",
       "·\n",
       "Jan 16, 2024\n",
       "--\n",
       "5\n",
       "Tomaz Bratanic\n",
       "in\n",
       "Neo4j Developer Blog\n",
       "JSON-based Agents With Ollama &amp; LangChain\n",
       "Learn to implement a Mixtral agent that interacts with a graph database Neo4j through a semantic layer\n",
       "8 min read\n",
       "·\n",
       "6 days ago\n",
       "--\n",
       "2\n",
       "Lists\n",
       "Predictive Modeling w/ Python\n",
       "20 stories\n",
       "·\n",
       "968 saves\n",
       "Natural Language Processing\n",
       "1254 stories\n",
       "·\n",
       "737 saves\n",
       "AI Regulation\n",
       "6 stories\n",
       "·\n",
       "349 saves\n",
       "Practical Guides to Machine Learning\n",
       "10 stories\n",
       "·\n",
       "1150 saves\n",
       "NebulaGraph Database\n",
       "Graph RAG: Unleashing the Power of Knowledge Graphs with LLM\n",
       "In the era of information overload, sifting through vast amounts of data to provide accurate search results in an engaging and…\n",
       "6 min read\n",
       "·\n",
       "Sep 8, 2023\n",
       "--\n",
       "4\n",
       "Isaac Kargar\n",
       "Inside Transformers: An In-depth Look at the Game-Changing Machine Learning Architecture — Part 2\n",
       "Let’s dig deeper into some of these components now. I will explain positional encoding, self-attention, multi-head attention, and masked…\n",
       "·\n",
       "5 min read\n",
       "·\n",
       "Feb 14, 2024\n",
       "--\n",
       "1\n",
       "Plaban Nayak\n",
       "in\n",
       "AI Planet\n",
       "Implement RAG with Knowledge Graph and Llama-Index\n",
       "Hallucination is a common problem when working with large language models (LLMs). LLMs generate fluent and coherent text but often generate…\n",
       "25 min read\n",
       "·\n",
       "Dec 3, 2023\n",
       "--\n",
       "7\n",
       "Ashish Kumar\n",
       "Graph Attention Networks\n",
       "Graph Machine Learning\n",
       "18 min read\n",
       "·\n",
       "Oct 20, 2023\n",
       "--\n",
       "See more recommendations\n",
       "Help\n",
       "Status\n",
       "About\n",
       "Careers\n",
       "Blog\n",
       "Privacy\n",
       "Terms\n",
       "Text to speech\n",
       "Teams"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://towardsdatascience.com/graph-ml-in-2023-the-state-of-affairs-1ba920cb9232?source=collection_archive'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swlh_df_2023 = scraper(r\"https://medium.com/swlh/\",2023,'swlh_df_2023.csv')\n",
    "swlh_df_2022 = scraper(r\"https://medium.com/swlh/\",2022,'swlh_df_2022.csv')\n",
    "swlh_df_2021 = scraper(r\"https://medium.com/swlh/\",2021,'swlh_df_2021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://towardsdatascience.com//archive/2023/01/01\n"
     ]
    },
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=122.0.6261.95)\nStacktrace:\n\tGetHandleVerifier [0x00007FF61986AD22+56930]\n\t(No symbol) [0x00007FF6197DF622]\n\t(No symbol) [0x00007FF6196942E5]\n\t(No symbol) [0x00007FF619671D4C]\n\t(No symbol) [0x00007FF6197023F7]\n\t(No symbol) [0x00007FF619717891]\n\t(No symbol) [0x00007FF6196FBA43]\n\t(No symbol) [0x00007FF6196CD438]\n\t(No symbol) [0x00007FF6196CE4D1]\n\tGetHandleVerifier [0x00007FF619BE6AAD+3709933]\n\tGetHandleVerifier [0x00007FF619C3FFED+4075821]\n\tGetHandleVerifier [0x00007FF619C3817F+4043455]\n\tGetHandleVerifier [0x00007FF619909756+706710]\n\t(No symbol) [0x00007FF6197EB8FF]\n\t(No symbol) [0x00007FF6197E6AE4]\n\t(No symbol) [0x00007FF6197E6C3C]\n\t(No symbol) [0x00007FF6197D68F4]\n\tBaseThreadInitThunk [0x00007FFE4243257D+29]\n\tRtlUserThreadStart [0x00007FFE441EAA58+40]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m towards_df_2023 \u001b[38;5;241m=\u001b[39m \u001b[43mscraper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://towardsdatascience.com/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2023\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtowards_df_2023.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m towards_df_2022 \u001b[38;5;241m=\u001b[39m scraper(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://towardsdatascience.com/\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m2022\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtowards_df_2022.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m towards_df_2021 \u001b[38;5;241m=\u001b[39m scraper(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://towardsdatascience.com/\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m2021\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtowards_df_2021.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[14], line 44\u001b[0m, in \u001b[0;36mscraper\u001b[1;34m(ogurl, year, csv_save_url)\u001b[0m\n\u001b[0;32m     42\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mogurl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/archive/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmonth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mday\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(url)\n\u001b[1;32m---> 44\u001b[0m page \u001b[38;5;241m=\u001b[39m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(page\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     47\u001b[0m stories \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstreamItem streamItem--postPreview js-streamItem\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\znay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:356\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    355\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 356\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\znay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:347\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    345\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 347\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\znay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=122.0.6261.95)\nStacktrace:\n\tGetHandleVerifier [0x00007FF61986AD22+56930]\n\t(No symbol) [0x00007FF6197DF622]\n\t(No symbol) [0x00007FF6196942E5]\n\t(No symbol) [0x00007FF619671D4C]\n\t(No symbol) [0x00007FF6197023F7]\n\t(No symbol) [0x00007FF619717891]\n\t(No symbol) [0x00007FF6196FBA43]\n\t(No symbol) [0x00007FF6196CD438]\n\t(No symbol) [0x00007FF6196CE4D1]\n\tGetHandleVerifier [0x00007FF619BE6AAD+3709933]\n\tGetHandleVerifier [0x00007FF619C3FFED+4075821]\n\tGetHandleVerifier [0x00007FF619C3817F+4043455]\n\tGetHandleVerifier [0x00007FF619909756+706710]\n\t(No symbol) [0x00007FF6197EB8FF]\n\t(No symbol) [0x00007FF6197E6AE4]\n\t(No symbol) [0x00007FF6197E6C3C]\n\t(No symbol) [0x00007FF6197D68F4]\n\tBaseThreadInitThunk [0x00007FFE4243257D+29]\n\tRtlUserThreadStart [0x00007FFE441EAA58+40]\n"
     ]
    }
   ],
   "source": [
    "towards_df_2023 = scraper(r\"https://towardsdatascience.com/\", 2023,'towards_df_2023.csv')\n",
    "towards_df_2022 = scraper(r\"https://towardsdatascience.com/\", 2022,'towards_df_2022.csv')\n",
    "towards_df_2021 = scraper(r\"https://towardsdatascience.com/\", 2021,'towards_df_2021.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "def is_generated_by_ai(paragraph):\n",
    "    # Load the text classification pipeline\n",
    "    text_classifier = pipeline(\"text-classification\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "    # Classify the input paragraph\n",
    "    result = text_classifier(paragraph)\n",
    "\n",
    "    # You can adjust this threshold based on experimentation\n",
    "    confidence_threshold = 0.7\n",
    "\n",
    "    # Check if the label is consistent with AI-generated text\n",
    "    label = result[0]['label']\n",
    "    confidence = result[0]['score']\n",
    "    if label == 'LABEL_1' and confidence >= confidence_threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Example usage\n",
    "input_paragraph = \"This is an example paragraph.\"\n",
    "generated_by_ai = is_generated_by_ai(input_paragraph)\n",
    "\n",
    "if generated_by_ai:\n",
    "    print(\"The paragraph seems to be generated by AI.\")\n",
    "else:\n",
    "    print(\"The paragraph seems to be written by a human.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Hello-SimpleAI/chatgpt-detector-roberta\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"Hello-SimpleAI/chatgpt-detector-roberta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)\n",
    "pipe(\"I have generated some mock data in Python that is obviously not linearly separable. However, there is a clear pattern separating the two classes which gives us an indication that we could create a function that splits the data in a higher dimension. So how about instead of trying to classify this data in two dimensions, we map the data to a three-dimensional space, and try to classify there.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(\"Amelia sat on the porch, sipping her coffee, gazing at the old oak tree in her backyard. Its branches whispered tales of time, and today, they carried a peculiar message. As she listened, a small bird landed on her shoulder, its song harmonizing with the rustling leaves. Suddenly, a letter floated down from the tree, tied with a crimson ribbon. Intrigued, she opened it to find words penned by her late grandmother, a letter lost for decades. Through tears and laughter, the oak tree became the keeper of family stories, connecting generations in a dance of memories under the vast sky.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "options = Options()\n",
    "options.add_argument(\"user-data-dir=C:/Users/znay/AppData/Local/Google/Chrome/User Data\")\n",
    "options.add_argument(r'--profile-directory=Profile 6') #e.g. Profile 3\n",
    "options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "options.add_experimental_option(\"excludeSwitches\",[\"enable logging\"])\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable automation\"])\n",
    "options.add_argument(\"start-maximized\")\n",
    "\n",
    "driver = webdriver.Chrome(service = Service(executable_path=r\"C:\\Users\\znay\\Downloads\\chromedriver.exe\"), options = options)\n",
    "\n",
    "driver.get(r\"https://medium.com/swlh/five-simple-ways-to-drastically-boost-your-energy-26f5ec19ad81\")\n",
    "\n",
    "m = driver.find_element(By.XPATH, \"/html/body\").text\n",
    "\n",
    "soup = BeautifulSoup(m, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Write\n",
       "Top highlight\n",
       "Member-only story\n",
       "Five Simple Ways to Drastically Boost Your Energy\n",
       "Workers are tired — and they care about their well-being more than ever.\n",
       "Aytekin Tank\n",
       "·\n",
       "Follow\n",
       "Published in\n",
       "The Startup\n",
       "·\n",
       "6 min read\n",
       "·\n",
       "Jan 9, 2023\n",
       "712\n",
       "15\n",
       "Photo by Rachel McDermott on Unsplash\n",
       "In the wake of multiple crises in the past few years, entrepreneurs are now facing a new one: a human energy crisis. As it turns out, the workplace flexibility afforded by the pandemic came at a cost.\n",
       "As Kathleen Hogan, Microsoft’s Chief People Officer and EVP, writes, “Since the start of the pandemic, we’ve seen workday span increase more than 13% and after-hours and weekend work are up 28% and 14% respectively.”\n",
       "The result? Workers around the globe are exhausted. And at the same time, they care more about their health and well-being than ever. In Microsoft’s annual Work Trend Index, a survey of thousands of workers worldwide, 53% of respondents said they’re more likely to prioritize their health and well-being over work than before. Now more than ever, we don’t want work to consume all of our energy.\n",
       "As CEO of Jotform, I’d be lying if I said I wasn’t concerned for my energy — and our employees’ — at certain points during these last three years. And yet, we managed to release new products and continue to grow our team. Here are some expert-backed strategies for drastically improving your energy levels. They’ve worked for me and hopefully, they will for you and your team.\n",
       "Take a strategic approach to sleep\n",
       "The secret to improved cognitive performance, boosted learning ability, brighter mood, and better overall health lies in sleep. Getting enough shut-eye is one surefire way to improve your life — and your business. And yet, so many of us — 80% as reported by the New York Times — struggle to get enough.\n",
       "The solution, however, may be less straightforward than it seems. It’s not a matter of forcing yourself to bed before 9:00pm or committing to exercise before dawn, but rather, determining what kind of sleeper you are and committing to a schedule that honors your body’s natural sleep tendencies.\n",
       "There are five main kinds of sleepers, or chronotypes, ranging from strong evening (aka, night owl) to strong morning (aka, early bird). It all has to do with when your inner clock starts to produce melatonin — for an early bird, that might be around 6:30pm, while a night owl may be closer to midnight. You can take a quiz to figure out your chronotype and whether it fits with your actual sleep schedule. Then, consider what kind of changes you may want to make so that your chronotype and sleep schedule align more closely.\n",
       "Being strategic about your sleep can help you to get more of it without trying to rewire your natural rhythms.\n",
       "Focus on maintaining, not just gaining, energy\n",
       "When aiming to boost your energy, think of it more like a marathon than a race. Writing for Harvard Business Review, Elizabeth Grace Saunders offers four tips for sustaining energy over the long term.\n",
       "Along with standard advice like setting aside time for rest and recovery, Saunders recommends setting upper and lower boundaries for the work you need to do to advance toward a goal.\n",
       "Say your team is working on a new product launch. As any entrepreneur knows, the timeframe for a launch can be months or even years. In order to maintain your momentum until the big day, you’d want to set a number of hours to dedicate to the product launch — the most you could do without burning out and the least to keep it moving forward, while still carving out time for your everyday tasks. So, maybe the upper boundary is three hours and the lower boundary is an hour. And if you tend to work like crazy in the beginning and then lose steam, or, on the contrary, procrastinate and then scramble at the end, you’d take that into account, too, to try to balance out the workload.\n",
       "In the end, the goal is to accommodate, not totally override, your natural energy levels. And as Saunders writes, “The key to success at work and in life isn’t really starting strong, it’s staying strong.”\n",
       "Create team rituals\n",
       "According to Harvard Business Review, workload isn’t what leaves employees feeling depleted. Rather, it’s factors like lack of meaning and a sense of isolation. “Remote work has increased people’s sense of isolation, a particularly harmful source of energy depletion. Loneliness intensifies the stress of difficult challenges,” write authors Ron Carucci and Kathleen Hogan.\n",
       "A straightforward way to overcome your or your employees’ loneliness is to create team rituals. Schedule weekly coffee breaks (online or in-person) where colleagues can chat about non-work topics like family or upcoming holidays. Host a regular trivia night. Launch a book club. Or, follow the example of one global communications firm and plan monthly Time to Connect to talk through recent (non-work) events.\n",
       "These rituals cultivate a greater sense of connection between employees and leave them feeling energized — not to mention, they can be fun, too.\n",
       "Transcend corporate silos\n",
       "Companies have always faced the issue of corporate silos. Problems arise when the separate departments become so isolated that they pursue their own departmental goals above the common goals of the company. The uptick in remote work has only exacerbated the potential for silos. According to HBR, remote work has weakened cross-functional relationships by as much as 25%. They explain, “This can increase isolation and monotony, two energy-depleting experiences.”\n",
       "So, how do you overcome silos and preserve your energy?\n",
       "At Jotform, we work in cross-functional teams — usually a senior developer, front-end developer, back-end developer, designer, and CSS developer. Some teams also have a project or product manager. That prevents silos before they start forming — and does wonders for our teams’ engagement and creativity. HBR cites one company that started a “walk in their shoes” program,” wherein colleagues learn about each other’s jobs through weekly peer-mentoring sessions. When a new employee starts at Hive, the company asks two or three Hive employees from other teams to take them out to lunch.\n",
       "When people interact across departments, they get a better sense of the bigger picture and are ultimately revitalized.\n",
       "Focus more on outcomes — less on being ‘on’\n",
       "Author Anne Helen Petersen calls it “LARPing” — Live Action Role Playing. It’s the custom of trying to show how hard at work we are — by replying to every message away, holding unnecessary meetings, staying connected to Slack, arriving early, and staying late… even when it’s not necessary. Unsurprisingly, LARPing depletes our energy and leaves us with less energy to dedicate to our actual work.\n",
       "In a recent conversation with Petersen, Future Forum Vice-President Sheela Subramanian explained, “Part of the problem is that nearly half of employees feel the need to show that they’re working in addition to their actual work.” That’s why Subramanian thinks that it’s critical to “focus on what are the outcomes that people need to deliver rather than always being on.”\n",
       "What’s more, flexible scheduling can also curb employees’ tendency to LARP. Subramanian shared that while 80% of people want flexibility in where they work, about 94% want flexibility in when they work.\n",
       "A\n",
       "llowing people to complete their work according to the schedule that suits them eliminates a standard schedule and alleviates the pressure to appear “on” at any time.\n",
       "Taken together, these strategies — getting strategic sleep, maintaining energy, creating team rituals, transcending silos, and focusing on outcomes — will lead to better, rather than more, work.\n",
       "Thank you for reading. Feel free to say hi on Twitter here.\n",
       "Originally published at https://www.jotform.com on January 10, 2023.\n",
       "Startup\n",
       "Productivity\n",
       "Psychology\n",
       "Leadership\n",
       "Culture\n",
       "712\n",
       "15\n",
       "Written by Aytekin Tank\n",
       "63K Followers\n",
       "·\n",
       "Writer for \n",
       "The Startup\n",
       "Founder and CEO of www.jotform.com || Bestselling author of Automate Your Busywork. Find more at https://aytekintank.com/ (contact: AytekinTank@Jotform.com)\n",
       "Follow\n",
       "Help\n",
       "Status\n",
       "About\n",
       "Careers\n",
       "Blog\n",
       "Privacy\n",
       "Terms\n",
       "Text to speech\n",
       "Teams"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
