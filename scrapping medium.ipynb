{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper(ogurl, year,csv_save_url):\n",
    "    import pandas as pd\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "\n",
    "    stories_data = []\n",
    "\n",
    "    for month in range(1, 13):\n",
    "        if month in [1, 3, 5, 7, 8, 10, 12]:\n",
    "            n_days = 31\n",
    "        elif month in [4, 6, 9, 11]:\n",
    "            n_days = 30\n",
    "        else:\n",
    "            n_days = 28\n",
    "\n",
    "        for day in range(1, n_days + 1):\n",
    "\n",
    "            month, day = str(month), str(day)\n",
    "\n",
    "            if len(month) == 1:\n",
    "                month = f'0{month}'\n",
    "            if len(day) == 1:\n",
    "                day = f'0{day}'\n",
    "\n",
    "            date = f'{month}/{day}/{year}'\n",
    "            url = f'{ogurl}/archive/{year}/{month}/{day}'\n",
    "            print(url)\n",
    "            page = requests.get(url)\n",
    "            soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "            stories = soup.find_all('div', class_='streamItem streamItem--postPreview js-streamItem')\n",
    "            j=0\n",
    "            for story in stories:\n",
    "                j=0\n",
    "                try:\n",
    "                    each_story = []\n",
    "\n",
    "                    author_box = story.find('div', class_='postMetaInline u-floatLeft u-sm-maxWidthFullWidth')\n",
    "                    author_url = author_box.find('a')['href']\n",
    "\n",
    "                    try:\n",
    "                        reading_time = author_box.find('span', class_='readingTime')['title']\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "                    title = story.find('h3').text if story.find('h3') else '-'\n",
    "\n",
    "                    subtitle = story.find('h4').text if story.find('h4') else '-'\n",
    "\n",
    "                    if story.find('button', class_='button button--chromeless u-baseColor--buttonNormal'\n",
    "                                               ' js-multirecommendCountButton u-disablePointerEvents'):\n",
    "\n",
    "                        claps = story.find('button', class_='button button--chromeless u-baseColor--buttonNormal'\n",
    "                                                        ' js-multirecommendCountButton u-disablePointerEvents').text\n",
    "                    else:\n",
    "                        claps = 0\n",
    "\n",
    "                    if story.find('a', class_='button button--chromeless u-baseColor--buttonNormal'):\n",
    "                        responses = story.find('a', class_='button button--chromeless u-baseColor--buttonNormal').text\n",
    "                    else:\n",
    "                        responses = '0 responses'\n",
    "\n",
    "                    story_url = story.find('a', class_='button button--smaller button--chromeless u-baseColor--buttonNormal')[\n",
    "                    'href']\n",
    "\n",
    "                    reading_time = reading_time.split()[0]\n",
    "                    responses = responses.split()[0]\n",
    "\n",
    "                    story_page = requests.get(story_url)\n",
    "                    story_soup = BeautifulSoup(story_page.text, 'html.parser')\n",
    "\n",
    "                    sections = story_soup.find_all('section')\n",
    "                    story_paragraphs = []\n",
    "                    section_titles = []\n",
    "                    for section in sections:\n",
    "                        paragraphs = section.find_all('p')\n",
    "                        for paragraph in paragraphs:\n",
    "                            story_paragraphs.append(paragraph.text)\n",
    "\n",
    "                        subs = section.find_all('h1')\n",
    "                        for sub in subs:\n",
    "                            section_titles.append(sub.text)\n",
    "\n",
    "                    number_sections = len(section_titles)\n",
    "                    number_paragraphs = len(story_paragraphs)\n",
    "\n",
    "                    each_story.append(date)\n",
    "                    each_story.append(title)\n",
    "                    each_story.append(subtitle)\n",
    "                    each_story.append(claps)\n",
    "                    each_story.append(responses)\n",
    "                    each_story.append(author_url)\n",
    "                    each_story.append(story_url)\n",
    "                    each_story.append(reading_time)\n",
    "                    each_story.append(number_sections)\n",
    "                    each_story.append(section_titles)\n",
    "                    each_story.append(number_paragraphs)\n",
    "                    each_story.append(story_paragraphs)\n",
    "\n",
    "                    stories_data.append(each_story)\n",
    "                except:\n",
    "                    j+=1\n",
    "                    continue\n",
    "\n",
    "\n",
    "            print(f'{len(stories)-j} stories scraped on {date}.')\n",
    "\n",
    "    columns = ['date', 'title', 'subtitle', 'claps', 'responses', 'author_url', 'story_url',\n",
    "               'reading_time (mins)', 'number_sections', 'section_titles', 'number_paragraphs', 'paragraphs']\n",
    "\n",
    "    df = pd.DataFrame(stories_data, columns=columns)\n",
    "    df.to_csv(csv_save_url, sep='\\t', index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swlh_df_2023 = scraper(r\"https://medium.com/swlh/\",2023,'swlh_df_2023.csv')\n",
    "swlh_df_2022 = scraper(r\"https://medium.com/swlh/\",2022,'swlh_df_2022.csv')\n",
    "swlh_df_2021 = scraper(r\"https://medium.com/swlh/\",2021,'swlh_df_2021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "towards_df_2023 = scraper(r\"https://towardsdatascience.com/\", 2023,'towards_df_2023.csv')\n",
    "towards_df_2022 = scraper(r\"https://towardsdatascience.com/\", 2022,'towards_df_2022.csv')\n",
    "towards_df_2021 = scraper(r\"https://towardsdatascience.com/\", 2021,'towards_df_2021.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "def is_generated_by_ai(paragraph):\n",
    "    # Load the text classification pipeline\n",
    "    text_classifier = pipeline(\"text-classification\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "    # Classify the input paragraph\n",
    "    result = text_classifier(paragraph)\n",
    "\n",
    "    # You can adjust this threshold based on experimentation\n",
    "    confidence_threshold = 0.7\n",
    "\n",
    "    # Check if the label is consistent with AI-generated text\n",
    "    label = result[0]['label']\n",
    "    confidence = result[0]['score']\n",
    "    if label == 'LABEL_1' and confidence >= confidence_threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Example usage\n",
    "input_paragraph = \"This is an example paragraph.\"\n",
    "generated_by_ai = is_generated_by_ai(input_paragraph)\n",
    "\n",
    "if generated_by_ai:\n",
    "    print(\"The paragraph seems to be generated by AI.\")\n",
    "else:\n",
    "    print(\"The paragraph seems to be written by a human.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Hello-SimpleAI/chatgpt-detector-roberta\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"Hello-SimpleAI/chatgpt-detector-roberta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)\n",
    "pipe(\"I have generated some mock data in Python that is obviously not linearly separable. However, there is a clear pattern separating the two classes which gives us an indication that we could create a function that splits the data in a higher dimension. So how about instead of trying to classify this data in two dimensions, we map the data to a three-dimensional space, and try to classify there.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(\"Amelia sat on the porch, sipping her coffee, gazing at the old oak tree in her backyard. Its branches whispered tales of time, and today, they carried a peculiar message. As she listened, a small bird landed on her shoulder, its song harmonizing with the rustling leaves. Suddenly, a letter floated down from the tree, tied with a crimson ribbon. Intrigued, she opened it to find words penned by her late grandmother, a letter lost for decades. Through tears and laughter, the oak tree became the keeper of family stories, connecting generations in a dance of memories under the vast sky.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
